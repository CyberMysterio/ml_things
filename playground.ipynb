{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmihaila/ml_things/blob/master/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_4HB0wnCFrx"
      },
      "source": [
        "# Bert Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfbbMzBHDvn_",
        "outputId": "a663bafd-098c-46fc-c2d0-fca4f51fe684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import io\n",
        "\n",
        "table = r\"\"\"+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Architecture       | Shortcut name                                              | Details of the model                                                                                                                  |\n",
        "+====================+============================================================+=======================================================================================================================================+\n",
        "| BERT               | ``bert-base-uncased``                                      | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on lower-cased English text.                                                                                                |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-large-uncased``                                     | | 24-layer, 1024-hidden, 16-heads, 336M parameters.                                                                                   |\n",
        "|                    |                                                            | | Trained on lower-cased English text.                                                                                                |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-base-cased``                                        | | 12-layer, 768-hidden, 12-heads, 109M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on cased English text.                                                                                                      |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-large-cased``                                       | | 24-layer, 1024-hidden, 16-heads, 335M parameters.                                                                                   |\n",
        "|                    |                                                            | | Trained on cased English text.                                                                                                      |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-base-multilingual-uncased``                         | | (Original, not recommended) 12-layer, 768-hidden, 12-heads, 168M parameters.                                                        |\n",
        "|                    |                                                            | | Trained on lower-cased text in the top 102 languages with the largest Wikipedias                                                    |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/bert/blob/master/multilingual.md>`__).                                              |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-base-multilingual-cased``                           | | (New, **recommended**) 12-layer, 768-hidden, 12-heads, 179M parameters.                                                             |\n",
        "|                    |                                                            | | Trained on cased text in the top 104 languages with the largest Wikipedias                                                          |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/bert/blob/master/multilingual.md>`__).                                              |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-base-chinese``                                      | | 12-layer, 768-hidden, 12-heads, 103M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on cased Chinese Simplified and Traditional text.                                                                           |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-base-german-cased``                                 | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on cased German text by Deepset.ai                                                                                          |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on deepset.ai website <https://deepset.ai/german-bert>`__).                                                             |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-large-uncased-whole-word-masking``                  | | 24-layer, 1024-hidden, 16-heads, 336M parameters.                                                                                   |\n",
        "|                    |                                                            | | Trained on lower-cased English text using Whole-Word-Masking                                                                        |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/bert/#bert>`__).                                                                    |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-large-cased-whole-word-masking``                    | | 24-layer, 1024-hidden, 16-heads, 335M parameters.                                                                                   |\n",
        "|                    |                                                            | | Trained on cased English text using Whole-Word-Masking                                                                              |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/bert/#bert>`__).                                                                    |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-large-uncased-whole-word-masking-finetuned-squad``  | | 24-layer, 1024-hidden, 16-heads, 336M parameters.                                                                                   |\n",
        "|                    |                                                            | | The ``bert-large-uncased-whole-word-masking`` model fine-tuned on SQuAD                                                             |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see details of fine-tuning in the `example section <https://github.com/huggingface/transformers/tree/master/examples>`__).           |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-large-cased-whole-word-masking-finetuned-squad``    | | 24-layer, 1024-hidden, 16-heads, 335M parameters                                                                                    |\n",
        "|                    |                                                            | | The ``bert-large-cased-whole-word-masking`` model fine-tuned on SQuAD                                                               |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details of fine-tuning in the example section <https://huggingface.co/transformers/examples.html>`__)                           |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-base-cased-finetuned-mrpc``                         | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | The ``bert-base-cased`` model fine-tuned on MRPC                                                                                    |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details of fine-tuning in the example section <https://huggingface.co/transformers/examples.html>`__)                           |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-base-german-dbmdz-cased``                           | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on cased German text by DBMDZ                                                                                               |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on dbmdz repository <https://github.com/dbmdz/german-bert>`__).                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``bert-base-german-dbmdz-uncased``                         | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on uncased German text by DBMDZ                                                                                             |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on dbmdz repository <https://github.com/dbmdz/german-bert>`__).                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``cl-tohoku/bert-base-japanese``                           | | 12-layer, 768-hidden, 12-heads, 111M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on Japanese text. Text is tokenized with MeCab and WordPiece and this requires some extra dependencies,                     |\n",
        "|                    |                                                            | | `fugashi <https://github.com/polm/fugashi>`__ which is a wrapper around `MeCab <https://taku910.github.io/mecab/>`__.               |\n",
        "|                    |                                                            | | Use ``pip install transformers[\"ja\"]`` (or ``pip install -e .[\"ja\"]`` if you install from source) to install them.                  |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on cl-tohoku repository <https://github.com/cl-tohoku/bert-japanese>`__).                                               |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``cl-tohoku/bert-base-japanese-whole-word-masking``        | | 12-layer, 768-hidden, 12-heads, 111M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on Japanese text. Text is tokenized with MeCab and WordPiece and this requires some extra dependencies,                     |\n",
        "|                    |                                                            | | `fugashi <https://github.com/polm/fugashi>`__ which is a wrapper around `MeCab <https://taku910.github.io/mecab/>`__.               |\n",
        "|                    |                                                            | | Use ``pip install transformers[\"ja\"]`` (or ``pip install -e .[\"ja\"]`` if you install from source) to install them.                  |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on cl-tohoku repository <https://github.com/cl-tohoku/bert-japanese>`__).                                               |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``cl-tohoku/bert-base-japanese-char``                      | | 12-layer, 768-hidden, 12-heads, 90M parameters.                                                                                     |\n",
        "|                    |                                                            | | Trained on Japanese text. Text is tokenized into characters.                                                                        |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on cl-tohoku repository <https://github.com/cl-tohoku/bert-japanese>`__).                                               |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``cl-tohoku/bert-base-japanese-char-whole-word-masking``   | | 12-layer, 768-hidden, 12-heads, 90M parameters.                                                                                     |\n",
        "|                    |                                                            | | Trained on Japanese text using Whole-Word-Masking. Text is tokenized into characters.                                               |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on cl-tohoku repository <https://github.com/cl-tohoku/bert-japanese>`__).                                               |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``TurkuNLP/bert-base-finnish-cased-v1``                    | | 12-layer, 768-hidden, 12-heads, 125M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on cased Finnish text.                                                                                                      |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on turkunlp.org <http://turkunlp.org/FinBERT/>`__).                                                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``TurkuNLP/bert-base-finnish-uncased-v1``                  | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on uncased Finnish text.                                                                                                    |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on turkunlp.org <http://turkunlp.org/FinBERT/>`__).                                                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``wietsedv/bert-base-dutch-cased``                         | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | Trained on cased Dutch text.                                                                                                        |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details on wietsedv repository <https://github.com/wietsedv/bertje/>`__).                                                       |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| GPT                | ``openai-gpt``                                             | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | OpenAI GPT English model                                                                                                            |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| GPT-2              | ``gpt2``                                                   | | 12-layer, 768-hidden, 12-heads, 117M parameters.                                                                                    |\n",
        "|                    |                                                            | | OpenAI GPT-2 English model                                                                                                          |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``gpt2-medium``                                            | | 24-layer, 1024-hidden, 16-heads, 345M parameters.                                                                                   |\n",
        "|                    |                                                            | | OpenAI's Medium-sized GPT-2 English model                                                                                           |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``gpt2-large``                                             | | 36-layer, 1280-hidden, 20-heads, 774M parameters.                                                                                   |\n",
        "|                    |                                                            | | OpenAI's Large-sized GPT-2 English model                                                                                            |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``gpt2-xl``                                                | | 48-layer, 1600-hidden, 25-heads, 1558M parameters.                                                                                  |\n",
        "|                    |                                                            | | OpenAI's XL-sized GPT-2 English model                                                                                               |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Transformer-XL     | ``transfo-xl-wt103``                                       | | 18-layer, 1024-hidden, 16-heads, 257M parameters.                                                                                   |\n",
        "|                    |                                                            | | English model trained on wikitext-103                                                                                               |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| XLNet              | ``xlnet-base-cased``                                       | | 12-layer, 768-hidden, 12-heads, 110M parameters.                                                                                    |\n",
        "|                    |                                                            | | XLNet English model                                                                                                                 |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlnet-large-cased``                                      | | 24-layer, 1024-hidden, 16-heads, 340M parameters.                                                                                   |\n",
        "|                    |                                                            | | XLNet Large English model                                                                                                           |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| XLM                | ``xlm-mlm-en-2048``                                        | | 12-layer, 2048-hidden, 16-heads                                                                                                     |\n",
        "|                    |                                                            | | XLM English model                                                                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-mlm-ende-1024``                                      | | 6-layer, 1024-hidden, 8-heads                                                                                                       |\n",
        "|                    |                                                            | | XLM English-German model trained on the concatenation of English and German wikipedia                                               |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-mlm-enfr-1024``                                      | | 6-layer, 1024-hidden, 8-heads                                                                                                       |\n",
        "|                    |                                                            | | XLM English-French model trained on the concatenation of English and French wikipedia                                               |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-mlm-enro-1024``                                      | | 6-layer, 1024-hidden, 8-heads                                                                                                       |\n",
        "|                    |                                                            | | XLM English-Romanian Multi-language model                                                                                           |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-mlm-xnli15-1024``                                    | | 12-layer, 1024-hidden, 8-heads                                                                                                      |\n",
        "|                    |                                                            | | XLM Model pre-trained with MLM on the `15 XNLI languages <https://github.com/facebookresearch/XNLI>`__.                             |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-mlm-tlm-xnli15-1024``                                | | 12-layer, 1024-hidden, 8-heads                                                                                                      |\n",
        "|                    |                                                            | | XLM Model pre-trained with MLM + TLM on the `15 XNLI languages <https://github.com/facebookresearch/XNLI>`__.                       |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-clm-enfr-1024``                                      | | 6-layer, 1024-hidden, 8-heads                                                                                                       |\n",
        "|                    |                                                            | | XLM English-French model trained with CLM (Causal Language Modeling) on the concatenation of English and French wikipedia           |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-clm-ende-1024``                                      | | 6-layer, 1024-hidden, 8-heads                                                                                                       |\n",
        "|                    |                                                            | | XLM English-German model trained with CLM (Causal Language Modeling) on the concatenation of English and German wikipedia           |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-mlm-17-1280``                                        | | 16-layer, 1280-hidden, 16-heads                                                                                                     |\n",
        "|                    |                                                            | | XLM model trained with MLM (Masked Language Modeling) on 17 languages.                                                              |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-mlm-100-1280``                                       | | 16-layer, 1280-hidden, 16-heads                                                                                                     |\n",
        "|                    |                                                            | | XLM model trained with MLM (Masked Language Modeling) on 100 languages.                                                             |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| RoBERTa            | ``roberta-base``                                           | | 12-layer, 768-hidden, 12-heads, 125M parameters                                                                                     |\n",
        "|                    |                                                            | | RoBERTa using the BERT-base architecture                                                                                            |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/pytorch/fairseq/tree/master/examples/roberta>`__)                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``roberta-large``                                          | | 24-layer, 1024-hidden, 16-heads, 355M parameters                                                                                    |\n",
        "|                    |                                                            | | RoBERTa using the BERT-large architecture                                                                                           |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/pytorch/fairseq/tree/master/examples/roberta>`__)                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``roberta-large-mnli``                                     | | 24-layer, 1024-hidden, 16-heads, 355M parameters                                                                                    |\n",
        "|                    |                                                            | | ``roberta-large`` fine-tuned on `MNLI <http://www.nyu.edu/projects/bowman/multinli/>`__.                                            |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/pytorch/fairseq/tree/master/examples/roberta>`__)                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``distilroberta-base``                                     | | 6-layer, 768-hidden, 12-heads, 82M parameters                                                                                       |\n",
        "|                    |                                                            | | The DistilRoBERTa model distilled from the RoBERTa model `roberta-base` checkpoint.                                                 |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__)                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``roberta-base-openai-detector``                           | | 12-layer, 768-hidden, 12-heads, 125M parameters                                                                                     |\n",
        "|                    |                                                            | | ``roberta-base`` fine-tuned by OpenAI on the outputs of the 1.5B-parameter GPT-2 model.                                             |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/openai/gpt-2-output-dataset/tree/master/detector>`__)                                               |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``roberta-large-openai-detector``                          | | 24-layer, 1024-hidden, 16-heads, 355M parameters                                                                                    |\n",
        "|                    |                                                            | | ``roberta-large`` fine-tuned by OpenAI on the outputs of the 1.5B-parameter GPT-2 model.                                            |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/openai/gpt-2-output-dataset/tree/master/detector>`__)                                               |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| DistilBERT         | ``distilbert-base-uncased``                                | | 6-layer, 768-hidden, 12-heads, 66M parameters                                                                                       |\n",
        "|                    |                                                            | | The DistilBERT model distilled from the BERT model `bert-base-uncased` checkpoint                                                   |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__)                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``distilbert-base-uncased-distilled-squad``                | | 6-layer, 768-hidden, 12-heads, 66M parameters                                                                                       |\n",
        "|                    |                                                            | | The DistilBERT model distilled from the BERT model `bert-base-uncased` checkpoint, with an additional linear layer.                 |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__)                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``distilbert-base-cased``                                  | | 6-layer, 768-hidden, 12-heads, 65M parameters                                                                                       |\n",
        "|                    |                                                            | | The DistilBERT model distilled from the BERT model `bert-base-cased` checkpoint                                                     |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__)                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``distilbert-base-cased-distilled-squad``                  | | 6-layer, 768-hidden, 12-heads, 65M parameters                                                                                       |\n",
        "|                    |                                                            | | The DistilBERT model distilled from the BERT model `bert-base-cased` checkpoint, with an additional question answering layer.       |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__)                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``distilgpt2``                                             | | 6-layer, 768-hidden, 12-heads, 82M parameters                                                                                       |\n",
        "|                    |                                                            | | The DistilGPT2 model distilled from the GPT2 model `gpt2` checkpoint.                                                               |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__)                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``distilbert-base-german-cased``                           | | 6-layer, 768-hidden, 12-heads, 66M parameters                                                                                       |\n",
        "|                    |                                                            | | The German DistilBERT model distilled from the German DBMDZ BERT model `bert-base-german-dbmdz-cased` checkpoint.                   |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__)                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``distilbert-base-multilingual-cased``                     | | 6-layer, 768-hidden, 12-heads, 134M parameters                                                                                      |\n",
        "|                    |                                                            | | The multilingual DistilBERT model distilled from the Multilingual BERT model `bert-base-multilingual-cased` checkpoint.             |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__)                                     |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| CTRL               | ``ctrl``                                                   | | 48-layer, 1280-hidden, 16-heads, 1.6B parameters                                                                                    |\n",
        "|                    |                                                            | | Salesforce's Large-sized CTRL English model                                                                                         |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| CamemBERT          | ``camembert-base``                                         | | 12-layer, 768-hidden, 12-heads, 110M parameters                                                                                     |\n",
        "|                    |                                                            | | CamemBERT using the BERT-base architecture                                                                                          |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/pytorch/fairseq/tree/master/examples/camembert>`__)                                                 |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| ALBERT             | ``albert-base-v1``                                         | | 12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters                                                            |\n",
        "|                    |                                                            | | ALBERT base model                                                                                                                   |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/ALBERT>`__)                                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``albert-large-v1``                                        | | 24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 17M parameters                                                           |\n",
        "|                    |                                                            | | ALBERT large model                                                                                                                  |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/ALBERT>`__)                                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``albert-xlarge-v1``                                       | | 24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M parameters                                                           |\n",
        "|                    |                                                            | | ALBERT xlarge model                                                                                                                 |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/ALBERT>`__)                                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``albert-xxlarge-v1``                                      | | 12 repeating layer, 128 embedding, 4096-hidden, 64-heads, 223M parameters                                                           |\n",
        "|                    |                                                            | | ALBERT xxlarge model                                                                                                                |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/ALBERT>`__)                                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``albert-base-v2``                                         | | 12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters                                                            |\n",
        "|                    |                                                            | | ALBERT base model with no dropout, additional training data and longer training                                                     |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/ALBERT>`__)                                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``albert-large-v2``                                        | | 24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 17M parameters                                                           |\n",
        "|                    |                                                            | | ALBERT large model with no dropout, additional training data and longer training                                                    |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/ALBERT>`__)                                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``albert-xlarge-v2``                                       | | 24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M parameters                                                           |\n",
        "|                    |                                                            | | ALBERT xlarge model with no dropout, additional training data and longer training                                                   |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/ALBERT>`__)                                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``albert-xxlarge-v2``                                      | | 12 repeating layer, 128 embedding, 4096-hidden, 64-heads, 223M parameters                                                           |\n",
        "|                    |                                                            | | ALBERT xxlarge model with no dropout, additional training data and longer training                                                  |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/google-research/ALBERT>`__)                                                                         |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| T5                 | ``t5-small``                                               | | ~60M parameters with 6-layers, 512-hidden-state, 2048 feed-forward hidden-state, 8-heads,                                           |\n",
        "|                    |                                                            | | Trained on English text: the Colossal Clean Crawled Corpus (C4)                                                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``t5-base``                                                | | ~220M parameters with 12-layers, 768-hidden-state, 3072 feed-forward hidden-state, 12-heads,                                        |\n",
        "|                    |                                                            | | Trained on English text: the Colossal Clean Crawled Corpus (C4)                                                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``t5-large``                                               | | ~770M parameters with 24-layers, 1024-hidden-state, 4096 feed-forward hidden-state, 16-heads,                                       |\n",
        "|                    |                                                            | | Trained on English text: the Colossal Clean Crawled Corpus (C4)                                                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``t5-3B``                                                  | | ~2.8B parameters with 24-layers, 1024-hidden-state, 16384 feed-forward hidden-state, 32-heads,                                      |\n",
        "|                    |                                                            | | Trained on English text: the Colossal Clean Crawled Corpus (C4)                                                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``t5-11B``                                                 | | ~11B parameters with 24-layers, 1024-hidden-state, 65536 feed-forward hidden-state, 128-heads,                                      |\n",
        "|                    |                                                            | | Trained on English text: the Colossal Clean Crawled Corpus (C4)                                                                     |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| XLM-RoBERTa        | ``xlm-roberta-base``                                       | | ~125M parameters with 12-layers, 768-hidden-state, 3072 feed-forward hidden-state, 8-heads,                                         |\n",
        "|                    |                                                            | | Trained on on 2.5 TB of newly created clean CommonCrawl data in 100 languages                                                       |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``xlm-roberta-large``                                      | | ~355M parameters with 24-layers, 1027-hidden-state, 4096 feed-forward hidden-state, 16-heads,                                       |\n",
        "|                    |                                                            | | Trained on 2.5 TB of newly created clean CommonCrawl data in 100 languages                                                          |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| FlauBERT           | ``flaubert/flaubert_small_cased``                          | | 6-layer, 512-hidden, 8-heads, 54M parameters                                                                                        |\n",
        "|                    |                                                            | | FlauBERT small architecture                                                                                                         |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/getalp/Flaubert>`__)                                                                                |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``flaubert/flaubert_base_uncased``                         | | 12-layer, 768-hidden, 12-heads, 137M parameters                                                                                     |\n",
        "|                    |                                                            | | FlauBERT base architecture with uncased vocabulary                                                                                  |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/getalp/Flaubert>`__)                                                                                |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``flaubert/flaubert_base_cased``                           | | 12-layer, 768-hidden, 12-heads, 138M parameters                                                                                     |\n",
        "|                    |                                                            | | FlauBERT base architecture with cased vocabulary                                                                                    |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/getalp/Flaubert>`__)                                                                                |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``flaubert/flaubert_large_cased``                          | | 24-layer, 1024-hidden, 16-heads, 373M parameters                                                                                    |\n",
        "|                    |                                                            | | FlauBERT large architecture                                                                                                         |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/getalp/Flaubert>`__)                                                                                |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Bart               | ``facebook/bart-large``                                    | | 24-layer, 1024-hidden, 16-heads, 406M parameters                                                                                    |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/pytorch/fairseq/tree/master/examples/bart>`_)                                                       |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``facebook/bart-base``                                     | | 12-layer, 768-hidden, 16-heads, 139M parameters                                                                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``facebook/bart-large-mnli``                               | | Adds a 2 layer classification head with 1 million parameters                                                                        |\n",
        "|                    |                                                            | | bart-large base architecture with a classification head, finetuned on MNLI                                                          |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``facebook/bart-large-cnn``                                | | 12-layer, 1024-hidden, 16-heads, 406M parameters       (same as base)                                                               |\n",
        "|                    |                                                            | | bart-large base architecture finetuned on cnn summarization task                                                                    |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| DialoGPT           | ``DialoGPT-small``                                         | | 12-layer, 768-hidden, 12-heads, 124M parameters                                                                                     |\n",
        "|                    |                                                            | | Trained on English text: 147M conversation-like exchanges extracted from Reddit.                                                    |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``DialoGPT-medium``                                        | | 24-layer, 1024-hidden, 16-heads, 355M parameters                                                                                    |\n",
        "|                    |                                                            | | Trained on English text: 147M conversation-like exchanges extracted from Reddit.                                                    |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``DialoGPT-large``                                         | | 36-layer, 1280-hidden, 20-heads, 774M parameters                                                                                    |\n",
        "|                    |                                                            | | Trained on English text: 147M conversation-like exchanges extracted from Reddit.                                                    |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Reformer           | ``reformer-enwik8``                                        | | 12-layer, 1024-hidden, 8-heads, 149M parameters                                                                                     |\n",
        "|                    |                                                            | | Trained on English Wikipedia data - enwik8.                                                                                         |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``reformer-crime-and-punishment``                          | | 6-layer, 256-hidden, 2-heads, 3M parameters                                                                                         |\n",
        "|                    |                                                            | | Trained on English text: Crime and Punishment novel by Fyodor Dostoyevsky.                                                          |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| MarianMT           | ``Helsinki-NLP/opus-mt-{src}-{tgt}``                       | | 12-layer, 512-hidden, 8-heads, ~74M parameter Machine translation models. Parameter counts vary depending on vocab size.            |\n",
        "|                    |                                                            | | (see `model list <https://huggingface.co/Helsinki-NLP>`_)                                                                           |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Pegasus            | ``google/pegasus-{dataset}``                               | | 16-layer, 1024-hidden, 16-heads, ~568M parameter, 2.2 GB for summary. `model list <https://huggingface.co/models?search=pegasus>`__ |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Longformer         | ``allenai/longformer-base-4096``                           | | 12-layer, 768-hidden, 12-heads, ~149M parameters                                                                                    |\n",
        "|                    |                                                            | | Starting from RoBERTa-base checkpoint, trained on documents of max length 4,096                                                     |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``allenai/longformer-large-4096``                          | | 24-layer, 1024-hidden, 16-heads, ~435M parameters                                                                                   |\n",
        "|                    |                                                            | | Starting from RoBERTa-large checkpoint, trained on documents of max length 4,096                                                    |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| MBart              | ``facebook/mbart-large-cc25``                              | | 24-layer, 1024-hidden, 16-heads, 610M parameters                                                                                    |\n",
        "|                    |                                                            | | mBART (bart-large architecture) model trained on 25 languages' monolingual corpus                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``facebook/mbart-large-en-ro``                             | | 24-layer, 1024-hidden, 16-heads, 610M parameters                                                                                    |\n",
        "|                    |                                                            | | mbart-large-cc25 model finetuned on WMT english romanian translation.                                                               |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Lxmert             | ``lxmert-base-uncased``                                    | | 9-language layers, 9-relationship layers, and 12-cross-modality layers                                                              |\n",
        "|                    |                                                            | | 768-hidden, 12-heads (for each layer) ~ 228M parameters                                                                             |\n",
        "|                    |                                                            | | Starting from lxmert-base checkpoint, trained on over 9 million image-text couplets from COCO, VisualGenome, GQA, VQA               |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| Funnel Transformer | ``funnel-transformer/small``                               | | 14 layers: 3 blocks of 4 layers then 2 layers decoder, 768-hidden, 12-heads, 130M parameters                                        |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/small-base``                          | | 12 layers: 3 blocks of 4 layers (no decoder), 768-hidden, 12-heads, 115M parameters                                                 |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/medium``                              | | 14 layers: 3 blocks 6, 3x2, 3x2 layers then 2 layers decoder, 768-hidden, 12-heads, 130M parameters                                 |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/medium-base``                         | | 12 layers: 3 blocks 6, 3x2, 3x2 layers(no decoder), 768-hidden, 12-heads, 115M parameters                                           |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/intermediate``                        | | 20 layers: 3 blocks of 6 layers then 2 layers decoder, 768-hidden, 12-heads, 177M parameters                                        |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/intermediate-base``                   | | 18 layers: 3 blocks of 6 layers (no decoder), 768-hidden, 12-heads, 161M parameters                                                 |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/large``                               | | 26 layers: 3 blocks of 8 layers then 2 layers decoder, 1024-hidden, 12-heads, 386M parameters                                       |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/large-base``                          | | 24 layers: 3 blocks of 8 layers (no decoder), 1024-hidden, 12-heads, 358M parameters                                                |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/xlarge``                              | | 32 layers: 3 blocks of 10 layers then 2 layers decoder, 1024-hidden, 12-heads, 468M parameters                                      |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``funnel-transformer/xlarge-base``                         | | 30 layers: 3 blocks of 10 layers (no decoder), 1024-hidden, 12-heads, 440M parameters                                               |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/laiguokun/Funnel-Transformer>`__)                                                                   |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| LayoutLM           | ``microsoft/layoutlm-base-uncased``                        | | 12 layers, 768-hidden, 12-heads, 113M parameters                                                                                    |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/microsoft/unilm/tree/master/layoutlm>`__)                                                           |\n",
        "+                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``microsoft/layoutlm-large-uncased``                       | | 24 layers, 1024-hidden, 16-heads, 343M parameters                                                                                   |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/microsoft/unilm/tree/master/layoutlm>`__)                                                           |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| DeBERTa            | ``microsoft/deberta-base``                                 | | 12-layer, 768-hidden, 12-heads, ~125M parameters                                                                                    |\n",
        "|                    |                                                            | | DeBERTa using the BERT-base architecture                                                                                            |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/microsoft/DeBERTa>`__)                                                                              |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``microsoft/deberta-large``                                | | 24-layer, 1024-hidden, 16-heads, ~390M parameters                                                                                   |\n",
        "|                    |                                                            | | DeBERTa using the BERT-large architecture                                                                                           |\n",
        "|                    |                                                            |                                                                                                                                       |\n",
        "|                    |                                                            | (see `details <https://github.com/microsoft/DeBERTa>`__)                                                                              |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "| SqueezeBERT        | ``squeezebert/squeezebert-uncased``                        | | 12-layer, 768-hidden, 12-heads, 51M parameters, 4.3x faster than bert-base-uncased on a smartphone.                                 |\n",
        "|                    |                                                            | | SqueezeBERT architecture pretrained from scratch on masked language model (MLM) and sentence order prediction (SOP) tasks.          |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``squeezebert/squeezebert-mnli``                           | | 12-layer, 768-hidden, 12-heads, 51M parameters, 4.3x faster than bert-base-uncased on a smartphone.                                 |\n",
        "|                    |                                                            | | This is the squeezebert-uncased model finetuned on MNLI sentence pair classification task with distillation from electra-base.      |\n",
        "|                    +------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "|                    | ``squeezebert/squeezebert-mnli-headless``                  | | 12-layer, 768-hidden, 12-heads, 51M parameters, 4.3x faster than bert-base-uncased on a smartphone.                                 |\n",
        "|                    |                                                            | | This is the squeezebert-uncased model finetuned on MNLI sentence pair classification task with distillation from electra-base.      |\n",
        "|                    |                                                            | | The final classification layer is removed, so when you finetune, the final layer will be reinitialized.                             |\n",
        "+--------------------+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
        "\"\"\"\n",
        "\n",
        "table = table.splitlines()\n",
        "\n",
        "architecture = None\n",
        "name = None\n",
        "details = []\n",
        "lines = ''\n",
        "\n",
        "for line in table[2:]:\n",
        "  if '+' in line:\n",
        "    continue\n",
        "  # print(line.split('|'))\n",
        "  tmp_architecture = line.split('|')[1].strip()\n",
        "  tmp_name = line.split('|')[2].strip()\n",
        "  tmp_details = ' '.join(line.split('|')[3:])\n",
        "  tmp_details = tmp_details.strip()\n",
        "\n",
        "  details.append(tmp_details)\n",
        "  \n",
        "\n",
        "  if tmp_name:\n",
        "    name = tmp_name if name is None else name\n",
        "    if name != tmp_name:\n",
        "      details = ' '.join(details)\n",
        "      name = name[2:-2]\n",
        "      lines += '%s\\t%s\\t%s\\n'%(architecture, name, details)\n",
        "      print(architecture, name, details)\n",
        "      name = tmp_name\n",
        "      details = []\n",
        "    \n",
        "  \n",
        "  if tmp_architecture:\n",
        "    architecture = tmp_architecture\n",
        "    \n",
        "io.open('transformers_pretrained_models.txt', 'w', encoding='utf-8').write(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT bert-base-uncased 12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text. 24-layer, 1024-hidden, 16-heads, 336M parameters.\n",
            "BERT bert-large-uncased Trained on lower-cased English text. 12-layer, 768-hidden, 12-heads, 109M parameters.\n",
            "BERT bert-base-cased Trained on cased English text. 24-layer, 1024-hidden, 16-heads, 335M parameters.\n",
            "BERT bert-large-cased Trained on cased English text. (Original, not recommended) 12-layer, 768-hidden, 12-heads, 168M parameters.\n",
            "BERT bert-base-multilingual-uncased Trained on lower-cased text in the top 102 languages with the largest Wikipedias  (see `details <https://github.com/google-research/bert/blob/master/multilingual.md>`__). (New, **recommended**) 12-layer, 768-hidden, 12-heads, 179M parameters.\n",
            "BERT bert-base-multilingual-cased Trained on cased text in the top 104 languages with the largest Wikipedias  (see `details <https://github.com/google-research/bert/blob/master/multilingual.md>`__). 12-layer, 768-hidden, 12-heads, 103M parameters.\n",
            "BERT bert-base-chinese Trained on cased Chinese Simplified and Traditional text. 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
            "BERT bert-base-german-cased Trained on cased German text by Deepset.ai  (see `details on deepset.ai website <https://deepset.ai/german-bert>`__). 24-layer, 1024-hidden, 16-heads, 336M parameters.\n",
            "BERT bert-large-uncased-whole-word-masking Trained on lower-cased English text using Whole-Word-Masking  (see `details <https://github.com/google-research/bert/#bert>`__). 24-layer, 1024-hidden, 16-heads, 335M parameters.\n",
            "BERT bert-large-cased-whole-word-masking Trained on cased English text using Whole-Word-Masking  (see `details <https://github.com/google-research/bert/#bert>`__). 24-layer, 1024-hidden, 16-heads, 336M parameters.\n",
            "BERT bert-large-uncased-whole-word-masking-finetuned-squad The ``bert-large-uncased-whole-word-masking`` model fine-tuned on SQuAD  (see details of fine-tuning in the `example section <https://github.com/huggingface/transformers/tree/master/examples>`__). 24-layer, 1024-hidden, 16-heads, 335M parameters\n",
            "BERT bert-large-cased-whole-word-masking-finetuned-squad The ``bert-large-cased-whole-word-masking`` model fine-tuned on SQuAD  (see `details of fine-tuning in the example section <https://huggingface.co/transformers/examples.html>`__) 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
            "BERT bert-base-cased-finetuned-mrpc The ``bert-base-cased`` model fine-tuned on MRPC  (see `details of fine-tuning in the example section <https://huggingface.co/transformers/examples.html>`__) 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
            "BERT bert-base-german-dbmdz-cased Trained on cased German text by DBMDZ  (see `details on dbmdz repository <https://github.com/dbmdz/german-bert>`__). 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
            "BERT bert-base-german-dbmdz-uncased Trained on uncased German text by DBMDZ  (see `details on dbmdz repository <https://github.com/dbmdz/german-bert>`__). 12-layer, 768-hidden, 12-heads, 111M parameters.\n",
            "BERT cl-tohoku/bert-base-japanese Trained on Japanese text. Text is tokenized with MeCab and WordPiece and this requires some extra dependencies, `fugashi <https://github.com/polm/fugashi>`__ which is a wrapper around `MeCab <https://taku910.github.io/mecab/>`__. Use ``pip install transformers[\"ja\"]`` (or ``pip install -e .[\"ja\"]`` if you install from source) to install them.  (see `details on cl-tohoku repository <https://github.com/cl-tohoku/bert-japanese>`__). 12-layer, 768-hidden, 12-heads, 111M parameters.\n",
            "BERT cl-tohoku/bert-base-japanese-whole-word-masking Trained on Japanese text. Text is tokenized with MeCab and WordPiece and this requires some extra dependencies, `fugashi <https://github.com/polm/fugashi>`__ which is a wrapper around `MeCab <https://taku910.github.io/mecab/>`__. Use ``pip install transformers[\"ja\"]`` (or ``pip install -e .[\"ja\"]`` if you install from source) to install them.  (see `details on cl-tohoku repository <https://github.com/cl-tohoku/bert-japanese>`__). 12-layer, 768-hidden, 12-heads, 90M parameters.\n",
            "BERT cl-tohoku/bert-base-japanese-char Trained on Japanese text. Text is tokenized into characters.  (see `details on cl-tohoku repository <https://github.com/cl-tohoku/bert-japanese>`__). 12-layer, 768-hidden, 12-heads, 90M parameters.\n",
            "BERT cl-tohoku/bert-base-japanese-char-whole-word-masking Trained on Japanese text using Whole-Word-Masking. Text is tokenized into characters.  (see `details on cl-tohoku repository <https://github.com/cl-tohoku/bert-japanese>`__). 12-layer, 768-hidden, 12-heads, 125M parameters.\n",
            "BERT TurkuNLP/bert-base-finnish-cased-v1 Trained on cased Finnish text.  (see `details on turkunlp.org <http://turkunlp.org/FinBERT/>`__). 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
            "BERT TurkuNLP/bert-base-finnish-uncased-v1 Trained on uncased Finnish text.  (see `details on turkunlp.org <http://turkunlp.org/FinBERT/>`__). 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
            "BERT wietsedv/bert-base-dutch-cased Trained on cased Dutch text.  (see `details on wietsedv repository <https://github.com/wietsedv/bertje/>`__). 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
            "GPT openai-gpt OpenAI GPT English model 12-layer, 768-hidden, 12-heads, 117M parameters.\n",
            "GPT-2 gpt2 OpenAI GPT-2 English model 24-layer, 1024-hidden, 16-heads, 345M parameters.\n",
            "GPT-2 gpt2-medium OpenAI's Medium-sized GPT-2 English model 36-layer, 1280-hidden, 20-heads, 774M parameters.\n",
            "GPT-2 gpt2-large OpenAI's Large-sized GPT-2 English model 48-layer, 1600-hidden, 25-heads, 1558M parameters.\n",
            "GPT-2 gpt2-xl OpenAI's XL-sized GPT-2 English model 18-layer, 1024-hidden, 16-heads, 257M parameters.\n",
            "Transformer-XL transfo-xl-wt103 English model trained on wikitext-103 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
            "XLNet xlnet-base-cased XLNet English model 24-layer, 1024-hidden, 16-heads, 340M parameters.\n",
            "XLNet xlnet-large-cased XLNet Large English model 12-layer, 2048-hidden, 16-heads\n",
            "XLM xlm-mlm-en-2048 XLM English model 6-layer, 1024-hidden, 8-heads\n",
            "XLM xlm-mlm-ende-1024 XLM English-German model trained on the concatenation of English and German wikipedia 6-layer, 1024-hidden, 8-heads\n",
            "XLM xlm-mlm-enfr-1024 XLM English-French model trained on the concatenation of English and French wikipedia 6-layer, 1024-hidden, 8-heads\n",
            "XLM xlm-mlm-enro-1024 XLM English-Romanian Multi-language model 12-layer, 1024-hidden, 8-heads\n",
            "XLM xlm-mlm-xnli15-1024 XLM Model pre-trained with MLM on the `15 XNLI languages <https://github.com/facebookresearch/XNLI>`__. 12-layer, 1024-hidden, 8-heads\n",
            "XLM xlm-mlm-tlm-xnli15-1024 6-layer, 1024-hidden, 8-heads\n",
            "XLM xlm-clm-enfr-1024 XLM English-French model trained with CLM (Causal Language Modeling) on the concatenation of English and French wikipedia 6-layer, 1024-hidden, 8-heads\n",
            "XLM xlm-clm-ende-1024 XLM English-German model trained with CLM (Causal Language Modeling) on the concatenation of English and German wikipedia 16-layer, 1280-hidden, 16-heads\n",
            "XLM xlm-mlm-17-1280 XLM model trained with MLM (Masked Language Modeling) on 17 languages. 16-layer, 1280-hidden, 16-heads\n",
            "XLM xlm-mlm-100-1280 XLM model trained with MLM (Masked Language Modeling) on 100 languages. 12-layer, 768-hidden, 12-heads, 125M parameters\n",
            "RoBERTa roberta-base RoBERTa using the BERT-base architecture  (see `details <https://github.com/pytorch/fairseq/tree/master/examples/roberta>`__) 24-layer, 1024-hidden, 16-heads, 355M parameters\n",
            "RoBERTa roberta-large RoBERTa using the BERT-large architecture  (see `details <https://github.com/pytorch/fairseq/tree/master/examples/roberta>`__) 24-layer, 1024-hidden, 16-heads, 355M parameters\n",
            "RoBERTa roberta-large-mnli ``roberta-large`` fine-tuned on `MNLI <http://www.nyu.edu/projects/bowman/multinli/>`__.  (see `details <https://github.com/pytorch/fairseq/tree/master/examples/roberta>`__) 6-layer, 768-hidden, 12-heads, 82M parameters\n",
            "RoBERTa distilroberta-base The DistilRoBERTa model distilled from the RoBERTa model `roberta-base` checkpoint.  (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__) 12-layer, 768-hidden, 12-heads, 125M parameters\n",
            "RoBERTa roberta-base-openai-detector ``roberta-base`` fine-tuned by OpenAI on the outputs of the 1.5B-parameter GPT-2 model.  (see `details <https://github.com/openai/gpt-2-output-dataset/tree/master/detector>`__) 24-layer, 1024-hidden, 16-heads, 355M parameters\n",
            "RoBERTa roberta-large-openai-detector ``roberta-large`` fine-tuned by OpenAI on the outputs of the 1.5B-parameter GPT-2 model.  (see `details <https://github.com/openai/gpt-2-output-dataset/tree/master/detector>`__) 6-layer, 768-hidden, 12-heads, 66M parameters\n",
            "DistilBERT distilbert-base-uncased The DistilBERT model distilled from the BERT model `bert-base-uncased` checkpoint  (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__) 6-layer, 768-hidden, 12-heads, 66M parameters\n",
            "DistilBERT distilbert-base-uncased-distilled-squad The DistilBERT model distilled from the BERT model `bert-base-uncased` checkpoint, with an additional linear layer.  (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__) 6-layer, 768-hidden, 12-heads, 65M parameters\n",
            "DistilBERT distilbert-base-cased The DistilBERT model distilled from the BERT model `bert-base-cased` checkpoint  (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__) 6-layer, 768-hidden, 12-heads, 65M parameters\n",
            "DistilBERT distilbert-base-cased-distilled-squad The DistilBERT model distilled from the BERT model `bert-base-cased` checkpoint, with an additional question answering layer.  (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__) 6-layer, 768-hidden, 12-heads, 82M parameters\n",
            "DistilBERT distilgpt2 The DistilGPT2 model distilled from the GPT2 model `gpt2` checkpoint.  (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__) 6-layer, 768-hidden, 12-heads, 66M parameters\n",
            "DistilBERT distilbert-base-german-cased The German DistilBERT model distilled from the German DBMDZ BERT model `bert-base-german-dbmdz-cased` checkpoint.  (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__) 6-layer, 768-hidden, 12-heads, 134M parameters\n",
            "DistilBERT distilbert-base-multilingual-cased The multilingual DistilBERT model distilled from the Multilingual BERT model `bert-base-multilingual-cased` checkpoint.  (see `details <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__) 48-layer, 1280-hidden, 16-heads, 1.6B parameters\n",
            "CTRL ctrl Salesforce's Large-sized CTRL English model 12-layer, 768-hidden, 12-heads, 110M parameters\n",
            "CamemBERT camembert-base CamemBERT using the BERT-base architecture  (see `details <https://github.com/pytorch/fairseq/tree/master/examples/camembert>`__) 12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters\n",
            "ALBERT albert-base-v1 ALBERT base model  (see `details <https://github.com/google-research/ALBERT>`__) 24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 17M parameters\n",
            "ALBERT albert-large-v1 ALBERT large model  (see `details <https://github.com/google-research/ALBERT>`__) 24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M parameters\n",
            "ALBERT albert-xlarge-v1 ALBERT xlarge model  (see `details <https://github.com/google-research/ALBERT>`__) 12 repeating layer, 128 embedding, 4096-hidden, 64-heads, 223M parameters\n",
            "ALBERT albert-xxlarge-v1 ALBERT xxlarge model  (see `details <https://github.com/google-research/ALBERT>`__) 12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters\n",
            "ALBERT albert-base-v2 ALBERT base model with no dropout, additional training data and longer training  (see `details <https://github.com/google-research/ALBERT>`__) 24 repeating layers, 128 embedding, 1024-hidden, 16-heads, 17M parameters\n",
            "ALBERT albert-large-v2 ALBERT large model with no dropout, additional training data and longer training  (see `details <https://github.com/google-research/ALBERT>`__) 24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M parameters\n",
            "ALBERT albert-xlarge-v2 ALBERT xlarge model with no dropout, additional training data and longer training  (see `details <https://github.com/google-research/ALBERT>`__) 12 repeating layer, 128 embedding, 4096-hidden, 64-heads, 223M parameters\n",
            "ALBERT albert-xxlarge-v2 ALBERT xxlarge model with no dropout, additional training data and longer training  (see `details <https://github.com/google-research/ALBERT>`__) ~60M parameters with 6-layers, 512-hidden-state, 2048 feed-forward hidden-state, 8-heads,\n",
            "T5 t5-small Trained on English text: the Colossal Clean Crawled Corpus (C4) ~220M parameters with 12-layers, 768-hidden-state, 3072 feed-forward hidden-state, 12-heads,\n",
            "T5 t5-base Trained on English text: the Colossal Clean Crawled Corpus (C4) ~770M parameters with 24-layers, 1024-hidden-state, 4096 feed-forward hidden-state, 16-heads,\n",
            "T5 t5-large Trained on English text: the Colossal Clean Crawled Corpus (C4) ~2.8B parameters with 24-layers, 1024-hidden-state, 16384 feed-forward hidden-state, 32-heads,\n",
            "T5 t5-3B Trained on English text: the Colossal Clean Crawled Corpus (C4) ~11B parameters with 24-layers, 1024-hidden-state, 65536 feed-forward hidden-state, 128-heads,\n",
            "T5 t5-11B Trained on English text: the Colossal Clean Crawled Corpus (C4) ~125M parameters with 12-layers, 768-hidden-state, 3072 feed-forward hidden-state, 8-heads,\n",
            "XLM-RoBERTa xlm-roberta-base Trained on on 2.5 TB of newly created clean CommonCrawl data in 100 languages ~355M parameters with 24-layers, 1027-hidden-state, 4096 feed-forward hidden-state, 16-heads,\n",
            "XLM-RoBERTa xlm-roberta-large Trained on 2.5 TB of newly created clean CommonCrawl data in 100 languages 6-layer, 512-hidden, 8-heads, 54M parameters\n",
            "FlauBERT flaubert/flaubert_small_cased FlauBERT small architecture  (see `details <https://github.com/getalp/Flaubert>`__) 12-layer, 768-hidden, 12-heads, 137M parameters\n",
            "FlauBERT flaubert/flaubert_base_uncased FlauBERT base architecture with uncased vocabulary  (see `details <https://github.com/getalp/Flaubert>`__) 12-layer, 768-hidden, 12-heads, 138M parameters\n",
            "FlauBERT flaubert/flaubert_base_cased FlauBERT base architecture with cased vocabulary  (see `details <https://github.com/getalp/Flaubert>`__) 24-layer, 1024-hidden, 16-heads, 373M parameters\n",
            "FlauBERT flaubert/flaubert_large_cased FlauBERT large architecture  (see `details <https://github.com/getalp/Flaubert>`__) 24-layer, 1024-hidden, 16-heads, 406M parameters\n",
            "Bart facebook/bart-large  (see `details <https://github.com/pytorch/fairseq/tree/master/examples/bart>`_) 12-layer, 768-hidden, 16-heads, 139M parameters\n",
            "Bart facebook/bart-base Adds a 2 layer classification head with 1 million parameters\n",
            "Bart facebook/bart-large-mnli bart-large base architecture with a classification head, finetuned on MNLI 12-layer, 1024-hidden, 16-heads, 406M parameters       (same as base)\n",
            "Bart facebook/bart-large-cnn bart-large base architecture finetuned on cnn summarization task 12-layer, 768-hidden, 12-heads, 124M parameters\n",
            "DialoGPT DialoGPT-small Trained on English text: 147M conversation-like exchanges extracted from Reddit. 24-layer, 1024-hidden, 16-heads, 355M parameters\n",
            "DialoGPT DialoGPT-medium Trained on English text: 147M conversation-like exchanges extracted from Reddit. 36-layer, 1280-hidden, 20-heads, 774M parameters\n",
            "DialoGPT DialoGPT-large Trained on English text: 147M conversation-like exchanges extracted from Reddit. 12-layer, 1024-hidden, 8-heads, 149M parameters\n",
            "Reformer reformer-enwik8 Trained on English Wikipedia data - enwik8. 6-layer, 256-hidden, 2-heads, 3M parameters\n",
            "Reformer reformer-crime-and-punishment Trained on English text: Crime and Punishment novel by Fyodor Dostoyevsky. 12-layer, 512-hidden, 8-heads, ~74M parameter Machine translation models. Parameter counts vary depending on vocab size.\n",
            "MarianMT Helsinki-NLP/opus-mt-{src}-{tgt} (see `model list <https://huggingface.co/Helsinki-NLP>`_) 16-layer, 1024-hidden, 16-heads, ~568M parameter, 2.2 GB for summary. `model list <https://huggingface.co/models?search=pegasus>`__\n",
            "Pegasus google/pegasus-{dataset} 12-layer, 768-hidden, 12-heads, ~149M parameters\n",
            "Longformer allenai/longformer-base-4096 Starting from RoBERTa-base checkpoint, trained on documents of max length 4,096 24-layer, 1024-hidden, 16-heads, ~435M parameters\n",
            "Longformer allenai/longformer-large-4096 Starting from RoBERTa-large checkpoint, trained on documents of max length 4,096 24-layer, 1024-hidden, 16-heads, 610M parameters\n",
            "MBart facebook/mbart-large-cc25 mBART (bart-large architecture) model trained on 25 languages' monolingual corpus 24-layer, 1024-hidden, 16-heads, 610M parameters\n",
            "MBart facebook/mbart-large-en-ro mbart-large-cc25 model finetuned on WMT english romanian translation. 9-language layers, 9-relationship layers, and 12-cross-modality layers\n",
            "Lxmert lxmert-base-uncased 768-hidden, 12-heads (for each layer) ~ 228M parameters Starting from lxmert-base checkpoint, trained on over 9 million image-text couplets from COCO, VisualGenome, GQA, VQA 14 layers: 3 blocks of 4 layers then 2 layers decoder, 768-hidden, 12-heads, 130M parameters\n",
            "Funnel Transformer funnel-transformer/small  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 12 layers: 3 blocks of 4 layers (no decoder), 768-hidden, 12-heads, 115M parameters\n",
            "Funnel Transformer funnel-transformer/small-base  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 14 layers: 3 blocks 6, 3x2, 3x2 layers then 2 layers decoder, 768-hidden, 12-heads, 130M parameters\n",
            "Funnel Transformer funnel-transformer/medium  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 12 layers: 3 blocks 6, 3x2, 3x2 layers(no decoder), 768-hidden, 12-heads, 115M parameters\n",
            "Funnel Transformer funnel-transformer/medium-base  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 20 layers: 3 blocks of 6 layers then 2 layers decoder, 768-hidden, 12-heads, 177M parameters\n",
            "Funnel Transformer funnel-transformer/intermediate  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 18 layers: 3 blocks of 6 layers (no decoder), 768-hidden, 12-heads, 161M parameters\n",
            "Funnel Transformer funnel-transformer/intermediate-base  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 26 layers: 3 blocks of 8 layers then 2 layers decoder, 1024-hidden, 12-heads, 386M parameters\n",
            "Funnel Transformer funnel-transformer/large  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 24 layers: 3 blocks of 8 layers (no decoder), 1024-hidden, 12-heads, 358M parameters\n",
            "Funnel Transformer funnel-transformer/large-base  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 32 layers: 3 blocks of 10 layers then 2 layers decoder, 1024-hidden, 12-heads, 468M parameters\n",
            "Funnel Transformer funnel-transformer/xlarge  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 30 layers: 3 blocks of 10 layers (no decoder), 1024-hidden, 12-heads, 440M parameters\n",
            "Funnel Transformer funnel-transformer/xlarge-base  (see `details <https://github.com/laiguokun/Funnel-Transformer>`__) 12 layers, 768-hidden, 12-heads, 113M parameters\n",
            "LayoutLM microsoft/layoutlm-base-uncased  (see `details <https://github.com/microsoft/unilm/tree/master/layoutlm>`__) 24 layers, 1024-hidden, 16-heads, 343M parameters\n",
            "LayoutLM microsoft/layoutlm-large-uncased  (see `details <https://github.com/microsoft/unilm/tree/master/layoutlm>`__) 12-layer, 768-hidden, 12-heads, ~125M parameters\n",
            "DeBERTa microsoft/deberta-base DeBERTa using the BERT-base architecture  (see `details <https://github.com/microsoft/DeBERTa>`__) 24-layer, 1024-hidden, 16-heads, ~390M parameters\n",
            "DeBERTa microsoft/deberta-large DeBERTa using the BERT-large architecture  (see `details <https://github.com/microsoft/DeBERTa>`__) 12-layer, 768-hidden, 12-heads, 51M parameters, 4.3x faster than bert-base-uncased on a smartphone.\n",
            "SqueezeBERT squeezebert/squeezebert-uncased SqueezeBERT architecture pretrained from scratch on masked language model (MLM) and sentence order prediction (SOP) tasks. 12-layer, 768-hidden, 12-heads, 51M parameters, 4.3x faster than bert-base-uncased on a smartphone.\n",
            "SqueezeBERT squeezebert/squeezebert-mnli This is the squeezebert-uncased model finetuned on MNLI sentence pair classification task with distillation from electra-base. 12-layer, 768-hidden, 12-heads, 51M parameters, 4.3x faster than bert-base-uncased on a smartphone.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20587"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vfb5e48CWW3",
        "outputId": "d6b392c3-a686-48d4-e8a6-103d1f8bedb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Extra code.\n",
        "import io\n",
        "\n",
        "model_name = None\n",
        "# Read all pretrained models\n",
        "pretrained_models = io.open('transformers_pretrained_models.txt', mode='r', encoding='utf-8').read().splitlines()\n",
        "\n",
        "# Find next runnign model\n",
        "for model_index in range(len(pretrained_models)):\n",
        "  model_info = pretrained_models[model_index].split('\\t')\n",
        "  if len(model_info) == 3:\n",
        "    # found model not ran\n",
        "    model_architecture, model_name, model_details = model_info\n",
        "    break\n",
        "\n",
        "# Make sure to break if finished running all models.\n",
        "if model_name is None:\n",
        "  raise ValueError('Finished running all models!')\n",
        "\n",
        "# Add as failed. We don't know if it will succeed or not.\n",
        "pretrained_models[model_index] = 'Failed\\t%s\\t%s\\t%s'%(model_architecture, model_name, model_details)\n",
        "\n",
        "# Update file.\n",
        "io.open('transformers_pretrained_models.txt', mode='w', encoding='utf-8').write('\\n'.join(pretrained_models))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-77dc489e2241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpretrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Failed\\t%s\\t%s\\t%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Finished!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu0tEP73r-Rj",
        "outputId": "28ef9d54-2e5f-4a7d-f713-2b0270e62746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import io\n",
        "\n",
        "# markdown_table = ['|%s|%s|%s|\\n'%('Status', 'Architecture', 'Shortcut name'),\n",
        "#                   '|:-|:-|:-|\\n',\n",
        "#                   ]\n",
        "\n",
        "all_status = {'Worked':[], 'Failed':[]}\n",
        "outputs_models = io.open(file='/content/transformers_pretrained_models.txt', mode='r', encoding='utf-8').read().splitlines()\n",
        "\n",
        "for line in outputs_models:\n",
        "  status, architecutre, model, details = line.split('\\t')\n",
        "  markdown_table.append('|%s|%s|%s|\\n'%(status, architecutre, model))\n",
        "  all_status[status].append(model)\n",
        "  # break\n",
        "\n",
        "print('Worked')\n",
        "tmp = [\"`%s`\"%m for m in all_status['Worked']]\n",
        "\n",
        "print(', '.join(tmp))\n",
        "\n",
        "len(all_status['Failed'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Worked\n",
            "`bert-base-uncased`, `bert-large-uncased`, `bert-base-cased`, `bert-large-cased`, `bert-base-multilingual-uncased`, `bert-base-multilingual-cased`, `bert-base-chinese`, `bert-base-german-cased`, `bert-large-uncased-whole-word-masking`, `bert-large-cased-whole-word-masking`, `bert-large-uncased-whole-word-masking-finetuned-squad`, `bert-large-cased-whole-word-masking-finetuned-squad`, `bert-base-cased-finetuned-mrpc`, `bert-base-german-dbmdz-cased`, `bert-base-german-dbmdz-uncased`, `TurkuNLP/bert-base-finnish-cased-v1`, `TurkuNLP/bert-base-finnish-uncased-v1`, `wietsedv/bert-base-dutch-cased`, `xlnet-base-cased`, `xlnet-large-cased`, `xlm-mlm-en-2048`, `xlm-mlm-ende-1024`, `xlm-mlm-enfr-1024`, `xlm-mlm-enro-1024`, `xlm-mlm-xnli15-1024`, `xlm-mlm-tlm-xnli15-1024`, `xlm-clm-enfr-1024`, `xlm-clm-ende-1024`, `xlm-mlm-17-1280`, `roberta-base`, `roberta-large`, `distilroberta-base`, `roberta-base-openai-detector`, `roberta-large-openai-detector`, `distilbert-base-uncased`, `distilbert-base-uncased-distilled-squad`, `distilbert-base-cased`, `distilbert-base-cased-distilled-squad`, `distilbert-base-german-cased`, `distilbert-base-multilingual-cased`, `camembert-base`, `albert-base-v1`, `albert-large-v1`, `albert-xlarge-v1`, `albert-xxlarge-v1`, `albert-base-v2`, `albert-large-v2`, `albert-xlarge-v2`, `albert-xxlarge-v2`, `xlm-roberta-base`, `xlm-roberta-large`, `flaubert/flaubert_small_cased`, `flaubert/flaubert_base_uncased`, `flaubert/flaubert_base_cased`, `flaubert/flaubert_large_cased`, `facebook/bart-large`, `facebook/bart-base`, `facebook/bart-large-cnn`, `allenai/longformer-base-4096`, `allenai/longformer-large-4096`, `funnel-transformer/small`, `funnel-transformer/small-base`, `funnel-transformer/medium`, `funnel-transformer/medium-base`, `funnel-transformer/intermediate`, `funnel-transformer/intermediate-base`, `funnel-transformer/large`, `funnel-transformer/large-base`, `funnel-transformer/xlarge`, `funnel-transformer/xlarge-base`, `microsoft/deberta-base`, `microsoft/deberta-large`, `squeezebert/squeezebert-uncased`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YLJqLfotPAD"
      },
      "source": [
        "Worked: `bert-base-uncased`, `bert-large-uncased`, `bert-base-cased`, `bert-large-cased`, `bert-base-multilingual-uncased`, `bert-base-multilingual-cased`, `bert-base-chinese`, `bert-base-german-cased`, `bert-large-uncased-whole-word-masking`, `bert-large-cased-whole-word-masking`, `bert-large-uncased-whole-word-masking-finetuned-squad`, `bert-large-cased-whole-word-masking-finetuned-squad`, `bert-base-cased-finetuned-mrpc`, `bert-base-german-dbmdz-cased`, `bert-base-german-dbmdz-uncased`, `TurkuNLP/bert-base-finnish-cased-v1`, `TurkuNLP/bert-base-finnish-uncased-v1`, `wietsedv/bert-base-dutch-cased`, `xlnet-base-cased`, `xlnet-large-cased`, `xlm-mlm-en-2048`, `xlm-mlm-ende-1024`, `xlm-mlm-enfr-1024`, `xlm-mlm-enro-1024`, `xlm-mlm-xnli15-1024`, `xlm-mlm-tlm-xnli15-1024`, `xlm-clm-enfr-1024`, `xlm-clm-ende-1024`, `xlm-mlm-17-1280`, `roberta-base`, `roberta-large`, `distilroberta-base`, `roberta-base-openai-detector`, `roberta-large-openai-detector`, `distilbert-base-uncased`, `distilbert-base-uncased-distilled-squad`, `distilbert-base-cased`, `distilbert-base-cased-distilled-squad`, `distilbert-base-german-cased`, `distilbert-base-multilingual-cased`, `camembert-base`, `albert-base-v1`, `albert-large-v1`, `albert-xlarge-v1`, `albert-xxlarge-v1`, `albert-base-v2`, `albert-large-v2`, `albert-xlarge-v2`, `albert-xxlarge-v2`, `xlm-roberta-base`, `xlm-roberta-large`, `flaubert/flaubert_small_cased`, `flaubert/flaubert_base_uncased`, `flaubert/flaubert_base_cased`, `flaubert/flaubert_large_cased`, `facebook/bart-large`, `facebook/bart-base`, `facebook/bart-large-cnn`, `allenai/longformer-base-4096`, `allenai/longformer-large-4096`, `funnel-transformer/small`, `funnel-transformer/small-base`, `funnel-transformer/medium`, `funnel-transformer/medium-base`, `funnel-transformer/intermediate`, `funnel-transformer/intermediate-base`, `funnel-transformer/large`, `funnel-transformer/large-base`, `funnel-transformer/xlarge`, `funnel-transformer/xlarge-base`, `microsoft/deberta-base`, `microsoft/deberta-large`, `squeezebert/squeezebert-uncased`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq5gDOJMDJM3"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxsZOOpBW1ye",
        "outputId": "552295ce-9e35-46c2-8a01-b3b06117ebfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "import wandb\n",
        "\n",
        "\n",
        "# configuration of run - what parameters to use\n",
        "config_run = dict(\n",
        "    dataset = 'my_dataset',\n",
        "    model_architecture = 'bert-base-cased',\n",
        "    epchs = 2,\n",
        "    batches = 32,\n",
        "    learning_rate = 1e-5,\n",
        "    )\n",
        "\n",
        "# initialization wandb\n",
        "init_run = dict(\n",
        "    project = 'test',   # name of project\n",
        "    name = 'Real Run',  # name of current run\n",
        "    notes = 'Comments', # any comments\n",
        "    resume = False,     # if need to resume - for any updates of same run\n",
        "    config = config_run # configuration of parameters\n",
        "    )\n",
        "\n",
        "# initialize wandb\n",
        "wandb.init(**init_wandb);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1211\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.01MB of 0.01MB uploaded (0.00MB deduped)\r\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20200928_160606-2bvkr9yv/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20200928_160606-2bvkr9yv/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mReal Run\u001b[0m: \u001b[34mhttps://wandb.ai/gm0234/test/runs/2bvkr9yv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200928_160724-evoex792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mReal Run\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/gm0234/test\" target=\"_blank\">https://wandb.ai/gm0234/test</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/gm0234/test/runs/evoex792\" target=\"_blank\">https://wandb.ai/gm0234/test/runs/evoex792</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyfeeOATDviI"
      },
      "source": [
        "for i in range(10):\n",
        "  \n",
        "  wandb.log({'accuracy': 0.9*i, 'epoch': i})\n",
        "\n",
        "wandb.log({\"random\": wandb.Histogram([i])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aHht0-zDvTj"
      },
      "source": [
        "# Visualize single plot\n",
        "wandb.sklearn.plot_confusion_matrix(y_true, y_pred, target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRmHl3Bygb9X"
      },
      "source": [
        "wandb.log(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNjTuNp2kM3a",
        "outputId": "50f24bb2-cbaf-4c87-a346-98ef50f5b3de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "wandb.run.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzWLV0KcDvQU",
        "outputId": "a386e304-a55f-4380-aeb3-492851b283a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = [0, 1, 2, 2, 2]\n",
        "y_pred = [0, 1, 2, 2, 1]\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "\n",
        "report = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
        "\n",
        "print(report)\n",
        "\n",
        "report['class 0']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'class 0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'class 1': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, 'class 2': {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 3}, 'accuracy': 0.8, 'macro avg': {'precision': 0.8333333333333334, 'recall': 0.8888888888888888, 'f1-score': 0.8222222222222223, 'support': 5}, 'weighted avg': {'precision': 0.9, 'recall': 0.8, 'f1-score': 0.8133333333333332, 'support': 5}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8m9QgQ7nees",
        "outputId": "a290bc1e-52be-4345-c742-0d315e984c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def make_archive(source, destination):\n",
        "    base_name = '.'.join(destination.split('.')[:-1])\n",
        "    format = destination.split('.')[-1]\n",
        "    root_dir = os.path.dirname(source)\n",
        "    base_dir = os.path.basename(source.strip(os.sep))\n",
        "    print(base_name, format, root_dir, base_dir)\n",
        "    shutil.make_archive(base_name, format, root_dir, base_dir)\n",
        "    return destination\n",
        "\n",
        "make_archive('/content/mlruns/0/%s'%run_dir, '/content/%s.zip'%run_dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/34ff5c1757d545e18b32fe64c3246f5f zip /content/mlruns/0 34ff5c1757d545e18b32fe64c3246f5f\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/34ff5c1757d545e18b32fe64c3246f5f.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc3ACl0CXwNN",
        "outputId": "c13b72c7-6b60-4a35-9def-075a8bfa9458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import mlflow\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# end any previous mlflow started\n",
        "mlflow.end_run()\n",
        "\n",
        "# remove privous mlflow folder\n",
        "shutil.rmtree('/content/mlruns') if os.path.isdir('/content/mlruns') else None;\n",
        "\n",
        "print('Starting new mlflow run!')\n",
        "# strating new mlflow\n",
        "mlflow.start_run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new mlflow run!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ActiveRun: >"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EIDBiB7f_0t"
      },
      "source": [
        "# log tag\n",
        "mlflow.set_tag(\"version\", \"tf-keras\")\n",
        "\n",
        "# log parameters\n",
        "mlflow.log_param(\"epochs\", 2)\n",
        "mlflow.log_param(\"batch\", 2)\n",
        "mlflow.log_param(\"valid_split\", 1)\n",
        "mlflow.log_param(\"optimizer\", 2)\n",
        "mlflow.log_param(\"loss\", 2)\n",
        "\n",
        "\n",
        "[mlflow.log_metric(key='loss', value=value, step=index) for index, value in enumerate(range(10))];\n",
        "\n",
        "# log artifacts\n",
        "# mlflow.log_artifact(\"tf_keras_mlflow.py\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Aq7JDoW8ra"
      },
      "source": [
        "# stop mlflow\n",
        "mlflow.end_run()\n",
        "# get run directory name\n",
        "run_dir = os.listdir('/content/mlruns/0'); run_dir.remove('meta.yaml'); run_dir = run_dir[0]\n",
        "# make archive\n",
        "path_zip = shutil.make_archive('/content/mlruns/%s'%run_dir, 'zip')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAEgHjnbn3Ak",
        "outputId": "2d9bb4bf-9114-402a-9b67-5d02b34ac074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "source": [
        "make_archive('mlruns/%s'%run_dir, '/content/%s.zip'%run_dir)\n",
        "    \n",
        "# download locally\n",
        "files.download(path_zip)\n",
        "\n",
        "# copy to drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-784450193639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mlruns/%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/%s.zip'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# download locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_zip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-fe74941a0442>\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(source, destination)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger)\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adding '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type)\u001b[0m\n\u001b[1;32m   1615\u001b[0m             )\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m         \u001b[0mzinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipInfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mzinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, filename, arcname)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0misdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mmtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '34ff5c1757d545e18b32fe64c3246f5f'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0IlBgdhdwyx",
        "outputId": "a1bfa52e-301e-472c-d5a0-bb07ac2021f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "run_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'60f9acfce02a483a912c06d52d11d6c5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3prDW_z2cRHO"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19QAg6rUWRvk"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixeMLTEWcmMB"
      },
      "source": [
        "# Lecture Students"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Uf4WDKcnou",
        "outputId": "17b66d7c-42ec-4c57-b07e-731036abddcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# lecture sections\n",
        "section_paths = {'001':'/content/2020-09-18T1837_Grades-CSCE_2110.001_(7210).csv',\n",
        "         '002':'/content/2020-09-19T1013_Grades-CSCE_2110.002_(15048).csv',\n",
        "         '004':'/content/2020-09-19T1013_Grades-CSCE_2110.004_(18759).csv',\n",
        "         '005':'/content/2020-09-19T1014_Grades-CSCE_2110.005_(18760).csv',\n",
        "}\n",
        "\n",
        "\n",
        "student_sec = {}\n",
        "euid_sec = {}\n",
        "\n",
        "for sec_num, sec_path in section_paths.items():\n",
        "\n",
        "  tmp_df = pd.read_csv(sec_path)\n",
        "\n",
        "  students = tmp_df['Student'].values[1:]\n",
        "  euids = tmp_df['SIS Login ID'].values[1:]\n",
        "\n",
        "  for student, euid in zip(students, euids):\n",
        "    student_sec[student] = sec_num\n",
        "    euid_sec[euid] = sec_num\n",
        "\n",
        "\n",
        "# add missing / innactive\n",
        "student_sec['Demoss, Harrison'] = '002'\n",
        "euid_sec['hd0162'] = '002'\n",
        "\n",
        "len(student_sec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBGmzpzYAI82"
      },
      "source": [
        "# Sec 281 Recitation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12QrdIpr9BVj",
        "outputId": "99b042f5-574c-4b12-c57f-3a18a72d4706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# zoom attendance\n",
        "participants = pd.read_csv('/content/participants_99210279764.csv')\n",
        "student_present = [', '.join(reversed(name.split())) for name in participants['Name (Original Name)'].values]\n",
        "\n",
        "# project master list\n",
        "df = pd.read_csv('/content/2110_sec281_proj1.csv')\n",
        "\n",
        "sec_281 = {'Student':df['Name'].values[0:],\n",
        "           'EUID':df['Unnamed: 1'].values[0:],\n",
        "           'Group ID':df['Group ID'].values[0:],\n",
        "           'Lecture Section':[student_sec[student] for student in df['Name'].values[0:]],\n",
        "           'Project 1 Expectation':['present']*len(df['Name'].values[0:]),\n",
        "           'Project 1 Design':['present' if name in student_present else 'absent' for name in df['Name'].values[0:]]}\n",
        "\n",
        "pd.DataFrame(sec_281).to_csv('2110_sec281_proj1_master.csv', index=False)\n",
        "\n",
        "# student missing from list - maybe misspelled name\n",
        "missing_student = [student for student in student_present if student not in df['Name'].values[0:]]\n",
        "missing_student"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mihaila, George',\n",
              " 'Alan',\n",
              " 'vanshikaganga',\n",
              " 'Aryan, Agarwal',\n",
              " 'Soto, Edwin',\n",
              " 'sterzenbach, ryan',\n",
              " 'abdelhamid, waleed',\n",
              " 'Ryan, Spencer#',\n",
              " 'Katta, Kumar, Sai, Tarun']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwOD83FYiGpe"
      },
      "source": [
        "Alan - Alan Mateo\n",
        "\n",
        "vanshikaganga - Vanshika Ganga\n",
        "\n",
        " Aryan Agarwal \n",
        "\n",
        " Soto-Villela, Edwin\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95esavXAAEJ9"
      },
      "source": [
        "# Sec 205"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrM34TYG64vG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1VT2PK2Fmzu"
      },
      "source": [
        "# DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-HyuifAeIgi"
      },
      "source": [
        "def create_db(db_name):\n",
        "  '''\n",
        "    CREATE DATABASE IF NOT EXISTS.\n",
        "  '''\n",
        " \n",
        "  try: \n",
        "    # CHECK IF DB EXISTS OR NOT\n",
        "    if not os.path.isfile(db_name):\n",
        "      # DB DOES NOT EXIST - CREATE DB\n",
        "      conn = sqlite3.connect(db_name)\n",
        "      # DB CONNECTION\n",
        "      c = conn.cursor()\n",
        "      # CREATE TABLE IN DB\n",
        "      c.execute('''CREATE TABLE jupyter_talon\n",
        "                  (euid, first_login, last_login, local_port, talon_port, login_node, count_logins, pid_session , state_session)''')\n",
        "      # SAVE (COMMIT) THE CHANGES\n",
        "      conn.commit()\n",
        "      # CLOSE CONNECITON\n",
        "      conn.close()\n",
        "      return True\n",
        "  except Exception as e:\n",
        "    print(\"DB FAILED!\", e)\n",
        "    return False\n",
        "\n",
        "\n",
        "def add_db(db_name, euid, last_login, local_port, talon_port, login_node, pid_session, state_session):\n",
        "  '''\n",
        "    ADD INSTANCE IN DATABASE\n",
        "    COLUMNS: 'euid', 'first_login', 'last_login', 'local_port', 'talon_port', 'login_node', 'count_logins', 'pid_session', 'state_session'\n",
        "    state_session:  'initiated' [login is started]\n",
        "                    'running'   [login is successfull]\n",
        "                    'ended'     [session ended]\n",
        "  '''\n",
        "  # DB CONNECTION\n",
        "  conn = None\n",
        "  # DB CURSOR\n",
        "  c = None\n",
        "  try: \n",
        "    # CHECK IF DB EXISTS\n",
        "    if os.path.isfile(db_name):\n",
        "      # DB EXISTS - JUST ESTABLISH CONNECTION\n",
        "      print('DataBase %s found!' % db_name)\n",
        "      # CONNECT TO DB\n",
        "      conn = sqlite3.connect(db_name)\n",
        "      # DB CONNECTION\n",
        "      c = conn.cursor()\n",
        "    else:\n",
        "      # DB DOES NOT EXIST\n",
        "      print(\"DataBase %s NOT found! Please run: 'create_db(db_name)'\" % db_name)\n",
        "      return\n",
        "\n",
        "    # CHECK IF EUID ALREADY EXISTS\n",
        "    euids = list(c.execute('SELECT euid FROM jupyter_talon ORDER BY euid').fetchall())\n",
        "    if (euid,) in euids:\n",
        "      # EUID ALREADY IN DB\n",
        "      print('USER IN DB')\n",
        "      # GRAB NUMBER OF LOGINS\n",
        "      count_logins = (c.execute('SELECT count_logins FROM jupyter_talon WHERE euid=?',(euid,)).fetchall())[0][0]\n",
        "      # ONLY COUNT AS LOGIN WHEN RUNNING\n",
        "      if state_session == 'running':\n",
        "        # INCREMENT LOGINS\n",
        "        count_logins += 1\n",
        "      # INCREMENT count_login\n",
        "      c.execute('UPDATE jupyter_talon SET last_login=?,\\\n",
        "                                          local_port=?,\\\n",
        "                                          talon_port=?,\\\n",
        "                                          login_node=?,\\\n",
        "                                          count_logins=?,\\\n",
        "                                          pid_session=?,\\\n",
        "                                          state_session=? WHERE euid=?',(\n",
        "                                          last_login, \n",
        "                                          local_port, \n",
        "                                          talon_port, \n",
        "                                          login_node,\n",
        "                                          count_logins, \n",
        "                                          pid_session, \n",
        "                                          state_session, \n",
        "                                          euid))\n",
        "    else:\n",
        "      # EUID NOT IN DB\n",
        "      # ADD EUID IN DB\n",
        "      first_login = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "      c.execute('INSERT INTO jupyter_talon VALUES (?,?,?,?,?,?,?,?,?)',(euid, \n",
        "                                                                      first_login, \n",
        "                                                                      last_login, \n",
        "                                                                      local_port, \n",
        "                                                                      talon_port, \n",
        "                                                                      login_node, \n",
        "                                                                      0,\n",
        "                                                                      pid_session, \n",
        "                                                                      state_session))\n",
        "    # SAVE (COMMIT) THE CHANGES\n",
        "    conn.commit()\n",
        "    # CLOSE CONNECITON\n",
        "    conn.close()\n",
        "  except Exception as e:\n",
        "    print(\"DB FAILED!\", e)\n",
        "  return\n",
        "\n",
        "\n",
        "def from_db(db_name, euid):\n",
        "  row = None\n",
        "  columns = ['first_login', 'last_login', 'local_port', 'talon_port', 'login_node', 'count_logins', 'pid_session', 'state_session']\n",
        "  try:\n",
        "    # CHECK IF DB EXISTS OR NOT\n",
        "    if os.path.isfile(db_name):\n",
        "      # CONNECT TO DB\n",
        "      conn = sqlite3.connect(db_name)\n",
        "      # DB CONNECTION\n",
        "      c = conn.cursor()\n",
        "      # CHECK IF EUID EXISTS\n",
        "      euids = list(c.execute('SELECT euid FROM jupyter_talon ORDER BY euid').fetchall())\n",
        "      if (euid,) in euids:\n",
        "        # EXTRACT ROW\n",
        "        row = list(c.execute('SELECT first_login,\\\n",
        "                                    last_login,\\\n",
        "                                    local_port,\\\n",
        "                                    talon_port,\\\n",
        "                                    login_node,\\\n",
        "                                    count_logins,\\\n",
        "                                    pid_session,\\\n",
        "                                    state_session FROM jupyter_talon WHERE euid=?',(euid,)).fetchall())\n",
        "        # DICTIONARY FORMAT\n",
        "        row = {k:v for k,v in zip(columns, row[0])}\n",
        "      else:\n",
        "        print('EUID %s NOT ADDED TO DB! ' % euid)\n",
        "    else:\n",
        "      print(\"DB %s does not exist!\" % db_name)\n",
        "  except Exception as e:\n",
        "    print(\"DB FAILED!\", e)\n",
        "  return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyngMuaCF8FW",
        "outputId": "05b70254-da90-4c82-b6e6-02cf7880090e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "import sqlite3\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "db_name = 'jupyter_talon_usage.db'\n",
        "\n",
        "create_db(db_name=db_name)\n",
        "\n",
        "add_db( db_name='jupyter_talon_usage.db',\n",
        "        euid = 'gm0234',\n",
        "        last_login = datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        local_port = 8888,\n",
        "        talon_port = 8888,\n",
        "        login_node = 'vis.acs.unt.edu',\n",
        "        pid_session = 356,\n",
        "        state_session = 'running')\n",
        "\n",
        "\n",
        "euid_log = from_db(db_name=db_name, euid='gm0234')\n",
        "\n",
        "euid_log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataBase jupyter_talon_usage.db found!\n",
            "USER IN DB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'count_logins': 7,\n",
              " 'first_login': '2020-02-24 23:20:53',\n",
              " 'last_login': '2020-02-24 23:26:14',\n",
              " 'local_port': 8888,\n",
              " 'login_node': 'vis.acs.unt.edu',\n",
              " 'pid_session': 356,\n",
              " 'state_session': 'running',\n",
              " 'talon_port': 8888}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PstcxP-PhPCH",
        "outputId": "f0df23e1-2ad7-4dbf-aa82-c8205ffe2b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "add_db( db_name='jupyter_talon_usage.db',\n",
        "        euid = 'gm0234',\n",
        "        last_login = datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        local_port = 82888,\n",
        "        talon_port = 8888,\n",
        "        login_node = 'vis.acs.unt.edu',\n",
        "        pid_session = 356,\n",
        "        state_session = 'running')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataBase jupyter_talon_usage.db found!\n",
            "USER IN DB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f60TTZHfgvig",
        "outputId": "b770cafc-5beb-46de-d0a4-d8c70cec0eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "get_ports(db_name='jupyter_talon_usage.db')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[82888, 8888, 8888, 8808, 8808, 8808, 8808, 8208]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X78ZramH6x1c"
      },
      "source": [
        "# Ports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0WU0kFcosG4"
      },
      "source": [
        "def get_ports(db_name):\n",
        "  # CHECK IF DB EXISTS OR NOT\n",
        "  if os.path.isfile(db_name):\n",
        "    try:\n",
        "      # CONNECT TO DB\n",
        "      conn = sqlite3.connect(db_name)\n",
        "      # DB CONNECTION\n",
        "      c = conn.cursor()\n",
        "      # CHECK IF EUID EXISTS\n",
        "      ports = list(c.execute('SELECT local_port FROM jupyter_talon').fetchall())\n",
        "      return [port[0] for port in ports]\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"DB READ FAILED!\", e)\n",
        "      return None\n",
        "\n",
        "  else:\n",
        "    print(\"DB %s DOES NOT EXIST!\" % db_name)\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNznfn5qjfea",
        "outputId": "2b685b59-5c3e-4a75-9d07-6e77c687ddc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def logger(user, message, level, fname='logs.log', verbose=True, extra_log=True):\n",
        "  \"\"\"Logging function\n",
        "\n",
        "  Args:\n",
        "    user: user id\n",
        "    message: text needed logged\n",
        "    level: 'CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG', 'NOTSET'\n",
        "    fname: file name to save all logs\n",
        "    verbose: if print to stdou\n",
        "    extra_log: create 'logs/' and write individual logs for each user\n",
        "\n",
        "  Source: https://docs.python.org/2/howto/logging.html\n",
        "  \"\"\"\n",
        "  \n",
        "  # get time of log\n",
        "  time_log = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "  # check input arguments types\n",
        "  assert level in ['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG', 'NOTSET']\n",
        "  assert str(user) and str(message)\n",
        "  # create log line from message and date\n",
        "  line = '%s %s %s %s'%(time_log, str(user), level, str(message))\n",
        "  # print to stdout if veborse\n",
        "  if verbose: print(line)\n",
        "  # append to main log file\n",
        "  with open(fname, 'a') as f:\n",
        "    f.write(line + '\\n')\n",
        "  if extra_log:\n",
        "    # create if folder does not exist\n",
        "    if os.path.isdir('logs') is False: os.mkdir('logs')\n",
        "    # append to user log file\n",
        "    with open('logs/%s.log'%user, 'a') as f:\n",
        "      f.write(line + '\\n')\n",
        "  return\n",
        "\n",
        "\n",
        "logger(user='gm0234', message='This is a test', level='DEBUG', verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-02-10 16:26:02 gm0234 DEBUG This is a test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqkC3WzLjd08",
        "outputId": "b223d28a-38a1-478c-b00d-c0bdbbab1f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if os.path.isdir('logs') is False:\n",
        "  os.mkdir('logs')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sdad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Dkd7TThhJw",
        "outputId": "ebff8bba-4da2-49cc-fd2c-b43efa749588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import logging\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  logging.basicConfig(filename='/content/my_logs.log', filemode='a', format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.DEBUG)\n",
        "\n",
        "  logging.debug('This message should appear on the console')\n",
        "  logging.info('So should this')\n",
        "  logging.warning('And this, too')\n",
        "  logging.error('this will')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_logs.log\n",
            "my_logs.log\n",
            "my_logs.log\n",
            "my_logs.log\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQrchisebvX-",
        "outputId": "01adf663-daeb-4ab0-85dc-35df1069792a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def tryPort(port):\n",
        "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    result = False\n",
        "    try:\n",
        "        sock.bind((\"0.0.0.0\", port))\n",
        "        result = True\n",
        "    except:\n",
        "        print(\"Port is in use\")\n",
        "    sock.close()\n",
        "    return result\n",
        "\n",
        "tryPort(8080)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Port is in use\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R155CL2yEgHI",
        "outputId": "7249567d-fe85-4559-b4d4-69c72ec2b16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def find_free_port(min_port=None, max_port=None):\n",
        "  if min_port and max_port:\n",
        "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    for port in range(min_port,max_port,1):\n",
        "      try:\n",
        "        sock.bind((\"0.0.0.0\", port))\n",
        "        sock.close()\n",
        "        return port\n",
        "        break\n",
        "      except:\n",
        "        continue\n",
        "  else:\n",
        "    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "        s.bind(('', 0))\n",
        "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "find_free_port(min_port=9000,max_port=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkD9Vj50cnbM",
        "outputId": "7e9c712f-ec3c-4c77-cdd5-48fb8a6bee47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "min_port=8080\n",
        "max_port=50000\n",
        "\n",
        "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "for port in range(min_port,max_port,1):\n",
        "  print('use',port)\n",
        "  try:\n",
        "    sock.bind((\"0.0.0.0\", port))\n",
        "    print(port)\n",
        "    sock.close()\n",
        "    break\n",
        "  except:\n",
        "    continue\n",
        "sock.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "use 8080\n",
            "use 8081\n",
            "8081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv-kHNnWZaWZ"
      },
      "source": [
        "import socket\n",
        "from contextlib import closing\n",
        "\n",
        "\n",
        "def find_free_port(min_port=None, max_port=None):\n",
        "  if min_port and max_port:\n",
        "    for port in range(min_port,max_port,1):\n",
        "      print(port)\n",
        "      with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "        s.bind(('', 0))\n",
        "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "        if s.getsockname()[1] == port:\n",
        "          return port\n",
        "\n",
        "  else:\n",
        "    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "        s.bind(('', 0))\n",
        "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "a = find_free_port(min_port=40000, max_port=50000)\n",
        "\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK1tm9V-uLNU",
        "outputId": "0079898a-dd1b-423a-8c65-ce03495dfae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import socket\n",
        "from contextlib import closing\n",
        "\n",
        "def find_free_port():\n",
        "    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "        s.bind(('', 0))\n",
        "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "a = find_free_port()\n",
        "\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK1pfNMPftpr",
        "outputId": "61507627-6e23-4559-9164-f0b6fcc524fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'djn%s'%None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'djnNone'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMINjq1rWTQY"
      },
      "source": [
        "def is_port_in_use(port):\n",
        "  import socket\n",
        "  with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "    return s.connect_ex(('localhost', port)) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuNjO4HDiimE"
      },
      "source": [
        "def port_used(port):\n",
        "  import socket, errno\n",
        "  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "  try:\n",
        "      s.bind((\"127.0.0.1\", port))\n",
        "  except socket.error as e:\n",
        "      if e.errno == errno.EADDRINUSE:\n",
        "          print(\"Port is already in use\")\n",
        "      else:\n",
        "          # something else raised the socket.error exception\n",
        "          print(e)\n",
        "  s.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MrOdjWESz5p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgBceghjg4Xj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt7r_HPVY8b_"
      },
      "source": [
        "python -c 'import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); print(\"port_used\",s.connect_ex((\"localhost\", 39634)) == 0)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2fK5OmjMyoN"
      },
      "source": [
        "# Encrypt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnt1AvjDM2J-",
        "outputId": "0669132a-afab-44d9-e4cf-f0da26c1e91f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import base64\n",
        "print(base64.b64encode(\"jupyter20$\".encode(\"utf-8\")))\n",
        "print(base64.b64decode(\"anVweXRlcjIwJA==\").decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'anVweXRlcjIwJA=='\n",
            "jupyter20$\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}