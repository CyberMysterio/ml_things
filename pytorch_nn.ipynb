{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmihaila/machine_learning_toolbox/blob/master/pytorch_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8f-8KN0CB46",
        "colab_type": "text"
      },
      "source": [
        "## SImple NN\n",
        "\n",
        "1 hiddent layer NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iddJzHDZCARo",
        "colab_type": "code",
        "outputId": "27576a6b-3369-481e-e403-2c2e19261cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "n_input, n_hidden, n_output = 5, 3, 1\n",
        "\n",
        "## initialize tensor for inputs, and outputs \n",
        "X = torch.randn((1, n_input))\n",
        "y = torch.rand((1,n_output)) \n",
        "\n",
        "\n",
        "print(x.size())\n",
        "print(y.size())\n",
        "print()\n",
        "\n",
        "## initialize tensor variables for weights \n",
        "w1 = torch.rand((n_input, n_hidden))\n",
        "w2 = torch.rand((n_hidden, n_output))\n",
        "\n",
        "print(w1.size())\n",
        "print(w2.size())\n",
        "print()\n",
        "\n",
        "## initialize tensor variables for bias terms \n",
        "b1 = torch.rand((1,n_hidden))\n",
        "b2 = torch.rand((1,n_output))\n",
        "\n",
        "print(b1.size())\n",
        "print(b2.size())\n",
        "print()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([1, 1])\n",
            "\n",
            "torch.Size([5, 3])\n",
            "torch.Size([3, 1])\n",
            "\n",
            "torch.Size([1, 3])\n",
            "torch.Size([1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzKELrKSD-t5",
        "colab_type": "text"
      },
      "source": [
        "1. Forward Propagation\n",
        "2. Loss computation\n",
        "3. Backpropagation\n",
        "4. Updating the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHNZuYc-EEWb",
        "colab_type": "code",
        "outputId": "4eb66562-2ba9-4f66-8c3f-6557a921d0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "## sigmoid activation function using pytorch\n",
        "def sigmoid_activation(z):\n",
        "  return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "## activation of hidden layer \n",
        "z1 = torch.mm(X,w1) + b1\n",
        "a1 = sigmoid_activation(z1)\n",
        "\n",
        "print(z1)\n",
        "print(a1)\n",
        "print()\n",
        "\n",
        "## activation (output) of final layer \n",
        "z2 = torch.mm(a1, w2) + b2\n",
        "a2 = output = sigmoid_activation(z2)\n",
        "\n",
        "print(z2)\n",
        "print(output)\n",
        "print()\n",
        "\n",
        "loss = y - output\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0190, -1.0797, -0.6849]])\n",
            "tensor([[0.4952, 0.2536, 0.3352]])\n",
            "\n",
            "tensor([[0.8171]])\n",
            "tensor([[0.6936]])\n",
            "\n",
            "tensor([[0.0193]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4oRU8O9JTOG",
        "colab_type": "text"
      },
      "source": [
        "### Backprop\n",
        "\n",
        "* loss gets multiplied by weights to penalize more of the bad weights\n",
        "* some weights contirbute more to the output. If the error is large, their loss will be more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbfW0NhaEkM_",
        "colab_type": "code",
        "outputId": "869f88a9-78a2-4112-a37d-d9ee024b19b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "## function to calculate the derivative of activation\n",
        "def sigmoid_delta(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "## compute derivative of error terms\n",
        "delta_output = sigmoid_delta(output)\n",
        "delta_hidden = sigmoid_delta(a1)\n",
        "\n",
        "print(delta_output)\n",
        "print(delta_hidden)\n",
        "print()\n",
        "\n",
        "\n",
        "## backpass the changes to previous layers \n",
        "d_output = loss * delta_output\n",
        "loss_h = torch.mm(d_output, w2.t())\n",
        "d_hidden = loss_h * delta_hidden\n",
        "\n",
        "print(d_output)\n",
        "print(loss_h)\n",
        "print(d_hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2125]])\n",
            "tensor([[0.2500, 0.1893, 0.2228]])\n",
            "\n",
            "tensor([[0.0041]])\n",
            "tensor([[0.0022, 0.0008, 0.0002]])\n",
            "tensor([[5.4456e-04, 1.5298e-04, 3.9821e-05]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbWmAwXtJ4xj",
        "colab_type": "text"
      },
      "source": [
        "### Update Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHRwQNWyJ-KG",
        "colab_type": "code",
        "outputId": "982ea3e1-fe8f-4936-9aab-9b811b1f005d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "w2 += torch.mm(a1.t(), d_output) * learning_rate\n",
        "w1 += torch.mm(X.t(), d_hidden) * learning_rate\n",
        "\n",
        "\n",
        "print(w2)\n",
        "print(w1)\n",
        "print()\n",
        "\n",
        "b1 += d_output.sum() * learning_rate\n",
        "b2 += d_hidden.sum() * learning_rate\n",
        "\n",
        "print(b1)\n",
        "print(b2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5323],\n",
            "        [0.1976],\n",
            "        [0.0441]])\n",
            "tensor([[0.4660, 0.4263, 0.1392],\n",
            "        [0.5357, 0.0522, 0.0833],\n",
            "        [0.0660, 0.7443, 0.5947],\n",
            "        [0.1371, 0.1337, 0.8427],\n",
            "        [0.7600, 0.7110, 0.8661]])\n",
            "\n",
            "tensor([[0.7842, 0.3612, 0.9707]])\n",
            "tensor([[0.4894]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdo5seVgay4f",
        "colab_type": "text"
      },
      "source": [
        "## MNIST -NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "65a63f53-5470-42b0-a60e-7c1185648297",
        "id": "ZSRKe2UUm9-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# transform the raw dataset into tensors and normalize them in a fixed range\n",
        "_tasks = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))]) #pass mean 0.5 and std 0.5\n",
        "\n",
        "## Load MNIST Dataset and apply transformations\n",
        "mnist = MNIST(\"data\", download=True, train=True, transform=_tasks)\n",
        "\n",
        "## create training and validation split \n",
        "split = int(0.8 * len(mnist))\n",
        "index_list = list(range(len(mnist)))\n",
        "train_idx, valid_idx = index_list[:split], index_list[split:]\n",
        "\n",
        "## create sampler objects using SubsetRandomSampler\n",
        "tr_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "## create iterator objects for train and valid datasets\n",
        "trainloader = DataLoader(mnist, batch_size=256, sampler=tr_sampler)\n",
        "validloader = DataLoader(mnist, batch_size=256, sampler=val_sampler)\n",
        "\n",
        "\n",
        "for data, label in trainloader:\n",
        "  print(np.shape(data))\n",
        "  # Flatten MNIST images into a 784 long vector\n",
        "  # data = data.view(data.shape[0], -1)\n",
        "  # print(data.shape)\n",
        "\n",
        "  data = torch.flatten(data, start_dim=1)\n",
        "  print(data.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RFHs8bda0xx",
        "colab_type": "text"
      },
      "source": [
        "### NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0coSr0H-a4yR",
        "colab_type": "code",
        "outputId": "120060f9-1187-49b9-fb4d-f984df8546e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# transform the raw dataset into tensors and normalize them in a fixed range\n",
        "_tasks = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "## Load MNIST Dataset and apply transformations\n",
        "mnist = MNIST(\"data\", download=True, train=True, transform=_tasks)\n",
        "\n",
        "## create training and validation split \n",
        "split = int(0.8 * len(mnist))\n",
        "index_list = list(range(len(mnist)))\n",
        "train_idx, valid_idx = index_list[:split], index_list[split:]\n",
        "\n",
        "## create sampler objects using SubsetRandomSampler\n",
        "tr_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "## create iterator objects for train and valid datasets\n",
        "trainloader = DataLoader(mnist, batch_size=256, sampler=tr_sampler)\n",
        "validloader = DataLoader(mnist, batch_size=256, sampler=val_sampler)\n",
        "\n",
        "## Build class of model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(784, 128)\n",
        "        self.output = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "model = Model()\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "for epoch in range(1,11):\n",
        "\n",
        "  train_loss, valid_loss = [], []\n",
        "  model.train() # activates training mod\n",
        "\n",
        "  ## Training on 1 epoch\n",
        "  for data, target in trainloader:\n",
        "\n",
        "    data = torch.flatten(data, start_dim=1)\n",
        "\n",
        "    optimizer.zero_grad() #clears gradients of all optimized classes\n",
        "\n",
        "    ## forward pass\n",
        "    output = model(data)\n",
        "\n",
        "    ## loss calc\n",
        "    loss = loss_function(output, target)\n",
        "\n",
        "    ## backeard propagation\n",
        "    loss.backward()\n",
        "\n",
        "    ## weight optimization\n",
        "    optimizer.step() #performs a single optimization step\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "  ### Evaluation on 1 epoch\n",
        "  for data, target in validloader:\n",
        "\n",
        "    data = torch.flatten(data, start_dim=1)\n",
        "\n",
        "    output = model(data)\n",
        "    loss = loss_function(output, target)\n",
        "    valid_loss.append(loss.item())\n",
        "  \n",
        "  print(\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Training Loss:  1.4278296362212364 Valid Loss:  0.7039066375570094\n",
            "Epoch: 2 Training Loss:  0.5753547265491588 Valid Loss:  0.446183591446978\n",
            "Epoch: 3 Training Loss:  0.43230179998468843 Valid Loss:  0.37309577490421053\n",
            "Epoch: 4 Training Loss:  0.3773977352266616 Valid Loss:  0.3384805096590773\n",
            "Epoch: 5 Training Loss:  0.34632741073344614 Valid Loss:  0.3146806723893957\n",
            "Epoch: 6 Training Loss:  0.3253439431019286 Valid Loss:  0.299487607593232\n",
            "Epoch: 7 Training Loss:  0.3091939938987823 Valid Loss:  0.28539510832187975\n",
            "Epoch: 8 Training Loss:  0.2957918224658104 Valid Loss:  0.27507571083434085\n",
            "Epoch: 9 Training Loss:  0.28438072516880136 Valid Loss:  0.2652054229315291\n",
            "Epoch: 10 Training Loss:  0.27375956244291144 Valid Loss:  0.2566439474516727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_5G0MzHewg8",
        "colab_type": "text"
      },
      "source": [
        "### Validation NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7DHUA_wbkYI",
        "colab_type": "code",
        "outputId": "7e164fe9-4ec5-4ca7-eb4c-565d9cd4cfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "## dataloader for validation dataset \n",
        "dataiter = iter(validloader)\n",
        "data, labels = dataiter.next()\n",
        "data = torch.flatten(data, start_dim=1)\n",
        "output = model(data)\n",
        "\n",
        "print(output.shape)\n",
        "print(output[0])\n",
        "\n",
        "_, pred_tensor = torch.max(output, 1)\n",
        "\n",
        "print(pred_tensor.shape)\n",
        "print(pred_tensor[0])\n",
        "\n",
        "preds = np.squeeze(pred_tensor.numpy())\n",
        "\n",
        "print(\"Actual: \", labels[:10])\n",
        "print(\"Predic: \", preds[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([256, 10])\n",
            "tensor([-3.2789, -1.0494,  0.6679,  0.9565, -1.4610,  1.6791, -3.1327, -2.5092,\n",
            "         8.5172,  0.7382], grad_fn=<SelectBackward>)\n",
            "torch.Size([256])\n",
            "tensor(8)\n",
            "Actual:  tensor([8, 8, 7, 9, 6, 8, 3, 7, 7, 5])\n",
            "Predic:  [8 5 7 9 6 8 8 7 7 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY07nwY8rcDg",
        "colab_type": "text"
      },
      "source": [
        "## MNIST - NN [1 GPU]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha61G2ngrbiT",
        "colab_type": "code",
        "outputId": "3b9e9339-8045-4bd8-d041-20f1e8f9ed5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "from torch.backends import cudnn\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# transform the raw dataset into tensors and normalize them in a fixed range\n",
        "_tasks = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "## Load MNIST Dataset and apply transformations\n",
        "mnist = MNIST(\"data\", download=True, train=True, transform=_tasks)\n",
        "\n",
        "## create training and validation split \n",
        "split = int(0.8 * len(mnist))\n",
        "index_list = list(range(len(mnist)))\n",
        "train_idx, valid_idx = index_list[:split], index_list[split:]\n",
        "\n",
        "## create sampler objects using SubsetRandomSampler\n",
        "tr_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "## create iterator objects for train and valid datasets\n",
        "trainloader = DataLoader(mnist, batch_size=256, sampler=tr_sampler, num_workers=2)\n",
        "validloader = DataLoader(mnist, batch_size=256, sampler=val_sampler, num_workers=2)\n",
        "\n",
        "## GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## Build class of model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(784, 128)\n",
        "        self.output = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "model = Model()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "for epoch in range(1,11):\n",
        "\n",
        "  train_loss, valid_loss = [], []\n",
        "  model.train() # activates training mod\n",
        "\n",
        "  ## Training on 1 epoch\n",
        "  for data, target in trainloader:\n",
        "\n",
        "    data = torch.flatten(data.to(device), start_dim=1)\n",
        "\n",
        "    optimizer.zero_grad() #clears gradients of all optimized classes\n",
        "\n",
        "    ## forward pass\n",
        "    output = model(data.to(device))\n",
        "\n",
        "    ## loss calc\n",
        "    loss = loss_function(output.to(device), target.to(device))\n",
        "\n",
        "    ## backeard propagation\n",
        "    loss.backward()\n",
        "\n",
        "    ## weight optimization\n",
        "    optimizer.step() #performs a single optimization step\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "  ### Evaluation on 1 epoch\n",
        "  for data, target in validloader:\n",
        "\n",
        "    data = torch.flatten(data, start_dim=1)\n",
        "\n",
        "    output = model(data.to(device))\n",
        "    loss = loss_function(output.to(device), target.to(device))\n",
        "    valid_loss.append(loss.item())\n",
        "  \n",
        "  print(\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scUaqYhqvfyP",
        "colab_type": "text"
      },
      "source": [
        "## MNIST - NN [Multy GPU - Multy Core]\n",
        "\n",
        "Specify certain GPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfdi1HANvi7f",
        "colab_type": "code",
        "outputId": "277e02e6-88b8-4d58-cc6e-1bb35b3a65e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\" # number of gpu devices\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "from torch.backends import cudnn\n",
        "import multiprocessing\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "n_cores = multiprocessing.cpu_count()\n",
        "\n",
        "# transform the raw dataset into tensors and normalize them in a fixed range\n",
        "_tasks = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "## Load MNIST Dataset and apply transformations\n",
        "mnist = MNIST(\"data\", download=True, train=True, transform=_tasks)\n",
        "\n",
        "## create training and validation split \n",
        "split = int(0.8 * len(mnist))\n",
        "index_list = list(range(len(mnist)))\n",
        "train_idx, valid_idx = index_list[:split], index_list[split:]\n",
        "\n",
        "## create sampler objects using SubsetRandomSampler\n",
        "tr_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "## create iterator objects for train and valid datasets\n",
        "trainloader = DataLoader(mnist, batch_size=256, sampler=tr_sampler, num_workers=n_cores)\n",
        "validloader = DataLoader(mnist, batch_size=256, sampler=val_sampler, num_workers=n_cores)\n",
        "\n",
        "## GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## Build class of model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.Linear(784, 128)\n",
        "        self.output = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "model = Model()\n",
        "\n",
        "## Multi GPU\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"We can use\", torch.cuda.device_count(), \"GPUs\")\n",
        "  model = nn.DataParallel(model, device_ids=[1]) # device_ids=[0,1,2] depending on the # of gpus\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "for epoch in range(1,11):\n",
        "\n",
        "  train_loss, valid_loss = [], []\n",
        "  model.train() # activates training mod\n",
        "\n",
        "  ## Training on 1 epoch\n",
        "  for data, target in trainloader:\n",
        "\n",
        "    data = torch.flatten(data.to(device), start_dim=1)\n",
        "\n",
        "    optimizer.zero_grad() #clears gradients of all optimized classes\n",
        "\n",
        "    ## forward pass\n",
        "    output = model(data.to(device))\n",
        "\n",
        "    ## loss calc\n",
        "    loss = loss_function(output.to(device), target.to(device))\n",
        "\n",
        "    ## backeard propagation\n",
        "    loss.backward()\n",
        "\n",
        "    ## weight optimization\n",
        "    optimizer.step() #performs a single optimization step\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "  ### Evaluation on 1 epoch\n",
        "  for data, target in validloader:\n",
        "\n",
        "    data = torch.flatten(data, start_dim=1)\n",
        "\n",
        "    output = model(data.to(device))\n",
        "    loss = loss_function(output.to(device), target.to(device))\n",
        "    valid_loss.append(loss.item())\n",
        "  \n",
        "  print(\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 9805960.78it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 130353.83it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2125030.13it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 49184.55it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Epoch: 1 Training Loss:  1.4125140620038865 Valid Loss:  0.7013939695155367\n",
            "Epoch: 2 Training Loss:  0.5760371469436808 Valid Loss:  0.447847954770352\n",
            "Epoch: 3 Training Loss:  0.4329018018981244 Valid Loss:  0.37440624389242616\n",
            "Epoch: 4 Training Loss:  0.37780846084686037 Valid Loss:  0.3380796021603523\n",
            "Epoch: 5 Training Loss:  0.3474148580051483 Valid Loss:  0.3164374764929426\n",
            "Epoch: 6 Training Loss:  0.32569479799651085 Valid Loss:  0.2996636686172891\n",
            "Epoch: 7 Training Loss:  0.3101388563184028 Valid Loss:  0.2881964070999876\n",
            "Epoch: 8 Training Loss:  0.2971356455632981 Valid Loss:  0.2755763013946249\n",
            "Epoch: 9 Training Loss:  0.2857853964446707 Valid Loss:  0.2682326946486818\n",
            "Epoch: 10 Training Loss:  0.27580255975431583 Valid Loss:  0.2596069701174472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wy3zgHCy7Ot",
        "colab_type": "text"
      },
      "source": [
        "## MNIST CNN [Multy GPU, CPU]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJUm3qjvBfFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}